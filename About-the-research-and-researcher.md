# About the Research and the Researcher
[◄ Go back](README.md)

I've provided some text below about the research and about myself, and also links to videos, academic papers and other resources.

## Videos and articles about the Research and the Researcher
I will make some up-to-date videos when I have time, here are links to some earlier videos about the research. They are each about 30-50 minutes long.

1. [Talk about the research from 2020: "Illusions, Magic and Test Automation"](https://www.youtube.com/watch?v=EAEG3CzZzVY) from Code Camp Romania.
2. [Report on the research from 2022: "Who is Testing? Do tools help and support them?"](https://www.youtube.com/watch?v=JhmZpnZO_ys) from TestMu conference
3. [Webinar from 2022 "Who is testing?"](https://www.bigmarker.com/techwell-corporation/Who-Is-Testing-A-Reflection-Part-Way-Through-Data-Analysis) from Techwell
4. [Interview with Agiletest](https://www.youtube.com/watch?v=0fZlOqjmZqQ) from Agiletest Invites.

I published [an article "Just a tester?"](https://www.womenwhotest.com/2023/04/14/just-a-tester-a-report-part-way-through-data/) in 2023 which summarises the research to that point and highlights of the findings.

## The Research - a brief history
###	Background
After nearly four decades working in the IT industry, mainly in software testing, latterly as a consultant and trainer, I had observed that teams and organizations didn’t have the successes they anticipated with adoption of tools to support testing. I wanted to understand the blockers to test tool adoption and what might overcome these, with an initial idea that improving the usability of tools would help people adopt them and use them successfully. I started a post-graduate study at University of Malta to research my ideas. 

###	Story So Far
Stories about experiences with test tools: In the initial stage of the research I went out to industry with a simple question “tell me a story about your experiences with test tools” via interviews, surveys and workshops, and was startled by the results. These were:
1.	Testers’ lived experience is adversely affected by poor tools (Papers: “[Stuck in Limbo with Magical Solutions](https://www.scitepress.org/Link.aspx?doi=10.5220/0009091801950202)” and “[Scared, Frustrated and Quietly Proud](https://dl.acm.org/doi/abs/10.1145/3452853.3452872)”).
2.	Usability was the major concern about the tools, followed by organizational impacts on tool use. All the other quality attributes of the tool made up about one third of the total concerns.
3.	Attempts to improve usability of tools had sometimes made the overall experience of the tool worse (Paper: “[Test Tools: The Illusion of Usability](https://ieeexplore.ieee.org/abstract/document/9155938)”):
    -	Usability was not enough by itself – other quality attributes are sometimes forgotten;
    -	Usability was often treated superficially by making the interface attractive but not improving workflows;
    -	Understanding of personas in testing was sometimes superficial – there was not an understanding of the range and type of people doing testing.

To better understand the potential in tester persona development, I asked "Who is testing?" in a further detailed survey which provided evidence of the heterogenous nature of the testing community, and the wide range of people’s characteristics, contexts, approaches, and requirements (Paper in draft). The data indicated that the people designing tools may benefit from a framework to help them better understand testers and the contexts for testing.

Based on the data analysed so far, I am working on building a set of heuristics for tool designers to use when designing test tools. I’m iterating through a series of reviews of versions of the heuristics both with UX experts and practitioners, and with test experts and practitioners. The draft heuristics are [listed on this repository](README.md), with their explanations and suggested activities.

First, I needed to understand how heuristics should be developed and what they look like:
1.	I examined literature (academic and industry) in the HCI/UX and Software Testing to look at how heuristics are developed, and also attended a BCS seminar for post graduate students on developing heuristics. I discovered that within HCI academic community several updates to Niesen’s Heuristics set have been published, together with critiques of the original and updated set, and also methods for developing and validating HCI heuristics.
2.	I examined literature (academic and industry) to see how heuristics are displayed and discovered that they have different levels of directedness and also that they are displayed in different formats. I defined a 3 by 3 grid to categorize the heuristic sets by “shape” that is a combination of their directedness and their display type (paper in draft). I'll share the grid later - only one part of it is relevant here.
3.	I collected examples of each, and then developed 3 slide sets. Each slide set had a single heuristic expressed in 9 different ways, and I interviewed UX practitioners and experts with the question: "Which shape of heuristic would be most useful to you when designing a software test tool?" The unanimous consensus was that a short question with underlying explanations is the most useful heuristic shape at the design stage of a tool. (NB: Nielsen’s heuristics are prompts. The consensus was that these are very useful when testing, and less useful when designing a software tool).
5.	I then set out to design a set of heuristic questions, grounded in the research data collected earlier.

### Current Activities and Next Steps:
I am in the middle of the iterative build-review-refining of the heuristic set content.
I have taken versions of the heuristics to test experts and practitioners to understand if the content of the heuristics indicate anything useful about the people who will use the test tools. That includes two mini case studies where I asked someone who had built a tool whether the heuristics would have changed their thinking when designing/building the tool, and a review with an accessibility expert.
I have updated the heuristics based on the review comments, and am now carrying out case studies to evaluate the heuristics in use.
Once the case studies have completed, I will refine the heuristics based on comments and observations of them in use, and publish.

This github repository is here to support the case studies and capture the heuristics and supporting information.

## The Researcher - a brief history

After more than 30 years in IT, on software industry projects, quality and testing practitioner I am now a part-time post-graduate student at the University of Malta. My research focuses on human factors in test automation. My interest in this topic arose from real-life experiences as a test manager, quality manager, and test consultant. I am an author, including a book, "Achieving Software Quality Through Teamwork", and chapters in "Agile Testing: How to Succeed in an eXtreme Testing Environment", "The Testing Practitioner", and "Foundations of Software Testing". I've spoken and told stories at software conferences worldwide, as well as chairing EuroSTAR (2019) and HUSTEF (2018). I am a Fellow of the British Computer Society and received the 2017 EuroSTAR Testing Excellence Award.

- [My linked-in profile](https://www.linkedin.com/in/isabelevans/)
- [My website](https://isabelevansuk.wordpress.com/)
- [My blog with consultancy stories](https://isabelevansconsultancy.wordpress.com/)
- [My other blog with stories about my interests](https://isabelevanswriting.wordpress.com/)

I do not use other social media. 
